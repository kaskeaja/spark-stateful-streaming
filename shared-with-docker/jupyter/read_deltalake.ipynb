{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed0aebb-d026-4caa-a724-46bd760bccb3",
   "metadata": {},
   "source": [
    "# Visualizing breakpoint transformation output \n",
    "\n",
    "## Usage:\n",
    "Run all the cells to visualize both breakpoint and processed data as well as to run processed data sanity checks.\n",
    "\n",
    "## Troubleshooting\n",
    "Sometimes Spark gets stuck or just fails to read the delta lake. Remedy is to restart the kernel or whole Jupyter lab. If restarting doesn't help then Delta Lake is corrupted for some reason and you need to remove source data, processed data or both. Before restarting writer and processor remove checkpoints too.\n",
    "\n",
    "* Source data in container file structure: */home/developer/shared_with_host/delta-lake-storage/delta-table-\\<deltalake_id\\>*\n",
    "* Processed data in container file structure: */home/developer/shared_with_host/delta-lake-storage/delta-table-processed-\\<deltalake_id\\>*\n",
    "* Checkpoints in container file structure: */tmp/checkpoint-\\<deltalake_id\\>*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514584cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import itertools\n",
    "\n",
    "from bokeh.models import ColumnDataSource, DatetimeTickFormatter, HoverTool, BoxSelectTool, WheelZoomTool, ResetTool\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import isnan\n",
    "\n",
    "def double_items(items):\n",
    "    return list(itertools.chain(*([[item, item] for item in items])))\n",
    "\n",
    "\n",
    "deltalake_id = '1'\n",
    "window_size = 1000\n",
    "timeseries_id='A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Breakpoint\n",
    "#\n",
    "\n",
    "df_breakpoint_deltalake = spark.read.format(\"delta\").load(f\"/home/developer/shared_with_host/delta-lake-storage/delta-table-{deltalake_id}\")\n",
    "df_breakpoint = df_breakpoint_deltalake.filter(df_breakpoint_deltalake.id == timeseries_id).toPandas().sort_values(by='eventTime')\n",
    "print(df_breakpoint_deltalake.show())\n",
    "\n",
    "\n",
    "#\n",
    "# Equally sampled\n",
    "#\n",
    "\n",
    "df_processed_deltalake = spark.read.format(\"delta\").load(f\"/home/developer/shared_with_host/delta-lake-storage/delta-table-processed-{deltalake_id}\")\n",
    "df_processed = df_processed_deltalake.filter(df_processed_deltalake.id == timeseries_id).toPandas().sort_values(by='windowStart')\n",
    "print(df_processed_deltalake.show())\n",
    "\n",
    "\n",
    "#\n",
    "# Plotting\n",
    "#\n",
    "\n",
    "breakpoint_source = ColumnDataSource(\n",
    "    data={\n",
    "        \"breakpoint_time\": df_breakpoint['eventTime'],\n",
    "        \"breakpoint_data\": df_breakpoint['data'],\n",
    "    }\n",
    ")\n",
    "\n",
    "processed_source = ColumnDataSource(\n",
    "    data={\n",
    "        \"processed_data_window_start\": [item + timedelta(seconds=0.5*1/float(window_size)) for item in df_processed[\"windowStart\"]],\n",
    "        \"processed_data\": df_processed['value'],\n",
    "    }\n",
    ")\n",
    "\n",
    "processed_point_source = ColumnDataSource(\n",
    "    data={\n",
    "        \"processed_data_window_start\": df_processed[\"windowStart\"],\n",
    "        \"processed_data\": df_processed['value'],\n",
    "    }\n",
    ")\n",
    "\n",
    "processed_step_curve = ColumnDataSource(\n",
    "    data = {\n",
    "        \"processed_data_window_start\" : double_items(df_processed[\"windowStart\"])[1:],\n",
    "        \"processed_data\": double_items(df_processed['value'])[:-1]\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "p = figure(\n",
    "    title=\"Breakpoint data to equally sampled data\",\n",
    "    x_axis_label='Time',\n",
    "    y_axis_label='value',\n",
    "    x_axis_type = \"datetime\",\n",
    ")\n",
    "\n",
    "p.plus(x='breakpoint_time', y='breakpoint_data', legend_label='Breakpoint', color='orange', source=breakpoint_source)\n",
    "p.square(x='processed_data_window_start', y='processed_data', legend_label='Processed point', color='red', source=processed_point_source)\n",
    "p.line(x='processed_data_window_start', y='processed_data', source=processed_step_curve)\n",
    "\n",
    "output_file(\"breakpoint_to_equally_sampled.html\")\n",
    "#p.plot_width=2400\n",
    "#p.plot_height=1200\n",
    "\n",
    "formatter = [\"%Y-%m-%d %H:%M:%S.%3Ns\"]\n",
    "p.xaxis.formatter=DatetimeTickFormatter(\n",
    "        seconds=formatter,\n",
    "        minutes=formatter,\n",
    "        hours=formatter,\n",
    "        days=formatter,\n",
    "        months=formatter,\n",
    "        years=formatter,\n",
    "    )\n",
    "\n",
    "show(p)\n",
    "\n",
    "plt.figure(figsize=(50, 10))\n",
    "plt.plot(\n",
    "    [item + timedelta(seconds=0.5*1/float(window_size)) for item in df_processed[\"windowStart\"]],\n",
    "    df_processed['value'],\n",
    "    's'\n",
    ")\n",
    "\n",
    "plt.plot(df_breakpoint['eventTime'], df_breakpoint['data'], 'x')\n",
    "plt.title('Breakpoint to equally sampled')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(['Breakpoint', 'Equally sampled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c808d954-cff3-40bd-a980-3a27b92806b6",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9eb71-99db-45a6-8f49-e4896437c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_processed_deltalake.filter(isnan(df_processed_deltalake.value)).count() == 0  # NaNs\n",
    "assert df_processed_deltalake.filter(df_processed_deltalake.value.isNull()).count() == 0 # Nulls\n",
    "assert df_processed_deltalake.filter(df_processed_deltalake.id=='A').groupBy(\"windowStart\").count().filter(\"count > 1\").count() == 0  # No duplicates test\n",
    "assert df_processed_deltalake.filter(df_processed_deltalake.id=='B').groupBy(\"windowStart\").count().filter(\"count > 1\").count() == 0  # No duplicates test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
